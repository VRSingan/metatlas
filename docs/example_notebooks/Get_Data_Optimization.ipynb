{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The time to beat is 60-80 seconds per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Metatlas live in ', '/global/homes/b/bpb/metatlas/metatlas')\n",
      "you're running on fb2ee5119e85 at 172.17.0.2 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, os\n",
    "sys.path.insert(0,'/global/homes/b/bpb/metatlas/' )\n",
    "sys.path.insert(1,'/global/project/projectdirs/metatlas/anaconda/lib/python2.7/site-packages' )\n",
    "import pandas as pd\n",
    "from metatlas import metatlas_objects as metob\n",
    "from metatlas.helpers import dill2plots as dp\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0 20160719_RL_SystemEval_QE144-UMLC_QC 2016-07-19 18:06:45\n",
      "1 20160719_RL_SystemEval_JGI-1290_QC 2016-07-19 18:06:49\n",
      "2 20160719_RL_SystemEval_JGI-1290_MeOH 2016-07-19 18:06:55\n",
      "3 20160719_RL_SystemEval_QE119-1290_MeOH 2016-07-19 18:06:58\n",
      "4 20160719_RL_SystemEval_QE119-1290_QC 2016-07-19 18:06:59\n",
      "5 20160719_RL_SystemEval_QE144-UMLC_MeOH 2016-07-19 18:07:02\n"
     ]
    }
   ],
   "source": [
    "dp = reload(dp)\n",
    "groups = dp.select_groups_for_analysis(name = '20160719_RL%',\n",
    "                                       most_recent = True,\n",
    "                                       remove_empty = True,\n",
    "                                       include_list = [])#, exclude_list = ['QC','Blank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconnecting to database\n",
      "5\n",
      "0 Avena_Hopland_KZ_RootExu_control_hilic_neg 2015-11-20 00:15:07\n",
      "1 Avena_Hopland_KZ_RootExu_3wk_hilic_neg 2015-11-20 00:15:06\n",
      "2 Avena_Hopland_KZ_RootExu_6wk_hilic_neg 2015-11-20 00:15:06\n",
      "3 Avena_Hopland_KZ_RootExu_9wk_hilic_neg 2015-11-20 00:15:07\n",
      "4 Avena_Hopland_KZ_RootExu_12wk_hilic_neg 2015-11-20 00:15:04\n"
     ]
    }
   ],
   "source": [
    "dp = reload(dp)\n",
    "groups = dp.select_groups_for_analysis(name = '%Hop%RootExu%neg%',\n",
    "                                       most_recent = True,\n",
    "                                       remove_empty = True,\n",
    "                                       include_list = [])#, exclude_list = ['QC','Blank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# my_groups = metob.retrieve('Groups',name = '%KZ%HA%pos')\n",
    "# print my_group.name, len(my_group.items)\n",
    "# my_file = my_group.items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11 2016719_JGIQE_QCs_C18_pos 2016-07-19 18:12:07\n"
     ]
    }
   ],
   "source": [
    "atlas = dp.get_metatlas_atlas(name='2016719_JG%',do_print = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 93 201500826_KZ_Ave_library_qxqct_hilic_neg_V2 2016-07-18 20:49:15\n"
     ]
    }
   ],
   "source": [
    "atlas = dp.get_metatlas_atlas(name='201500826_KZ_Ave_library_qxqct_hilic_neg%',do_print = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201500826_KZ_Ave_library_qxqct_hilic_neg_V2\n",
      "bpb\n"
     ]
    }
   ],
   "source": [
    "myAtlas = atlas[0]\n",
    "print myAtlas.name\n",
    "print myAtlas.username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atlas_identifications = dp.export_atlas_to_spreadsheet(myAtlas,'/global/homes/b/bpb/Downloads/KZ_Avena_Exudate_atlases_and groups_1/atlas_201500826_KZ_Ave_library_qxqct_hilic_neg_V2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def df_container_from_metatlas_file(my_file):\n",
    "    data_df = pd.DataFrame()\n",
    "    pd_h5_file  = pd.HDFStore(my_file.hdf5_file)\n",
    "    keys = pd_h5_file.keys()\n",
    "    pd_h5_file.close()\n",
    "    df_container = {}\n",
    "    for k in keys:\n",
    "        if ('ms' in k) and not ('_mz' in k):\n",
    "            new_df = pd.read_hdf(my_file.hdf5_file,k)\n",
    "            df_container[k[1:]] = new_df\n",
    "    return df_container\n",
    "\n",
    "def fast_nearest_interp(xi, x, y):\n",
    "    \"\"\"Assumes that x is monotonically increasing!!.\"\"\"\n",
    "    # Shift x points to centers\n",
    "    spacing = np.diff(x) / 2\n",
    "    x = x + np.hstack([spacing, spacing[-1]])\n",
    "    # Append the last point in y twice for ease of use\n",
    "    y = np.hstack([y, y[-1]])\n",
    "    return y[np.searchsorted(x, xi)]\n",
    "\n",
    "def remove_ms1_data_not_in_atlas(atlas,data):\n",
    "    things_to_do = [('positive','ms1_pos'),('negative','ms1_neg')]\n",
    "    for thing in things_to_do:\n",
    "        if sum(atlas_df.detected_polarity == thing[0])>0:\n",
    "            atlas_mz = atlas_df[atlas_df.detected_polarity == thing[0]].mz.copy()\n",
    "            atlas_mz = atlas_mz.sort_values().as_matrix()\n",
    "            max_mz_tolerance = atlas_df[atlas_df.detected_polarity == thing[0]].mz_tolerance.max()\n",
    "            if data[thing[1]].shape[0]>1:\n",
    "                original_mz = data[thing[1]].mz.as_matrix()\n",
    "                nearest_mz = fast_nearest_interp(original_mz,atlas_mz,atlas_mz)\n",
    "                data[thing[1]]['ppm_difference'] = abs(original_mz - nearest_mz) / original_mz * 1e6\n",
    "                query_str = 'ppm_difference < %f'%(max_mz_tolerance)\n",
    "                data[thing[1]] = data[thing[1]].query(query_str)\n",
    "    return data\n",
    "\n",
    "def make_atlas_df(atlas):\n",
    "    mz = []\n",
    "    rt = []\n",
    "    atlas_compound = []\n",
    "    label = []\n",
    "    for compound in atlas.compound_identifications:\n",
    "        if compound.mz_references:\n",
    "            mz.append(compound.mz_references[0])\n",
    "        else:\n",
    "            mz.append(metob.MzReference)\n",
    "        if compound.rt_references:\n",
    "            rt.append(compound.rt_references[0])\n",
    "        else:\n",
    "            rt.append(metob.RtReference)\n",
    "        if compound.compound:\n",
    "            atlas_compound.append(compound.compound[0])\n",
    "        else:\n",
    "            atlas_compound.append(metob.Compound())\n",
    "        label.append(compound.name)\n",
    "    compound_df = metob.to_dataframe(atlas_compound)\n",
    "    compound_df.rename(columns = {'name':'compound_name','description':'compound_description'},inplace=True)\n",
    "    #.rename(columns = {'name':'compound_name'}, inplace = True)\n",
    "    atlas_df = pd.concat([metob.to_dataframe(rt),metob.to_dataframe(mz), compound_df],axis=1)\n",
    "    # atlas_df['label'] = label\n",
    "\n",
    "    atlas_keys = [u'rt_max', u'rt_min', u'rt_peak',u'rt_units', u'detected_polarity', u'mz', u'mz_tolerance',u'mz_tolerance_units']\n",
    "\n",
    "    # atlas_keys = [u'label','compound_name','compound_description',u'synonyms', u'num_free_radicals', u'number_components', u'permanent_charge', u'rt_max', u'rt_min', u'rt_peak',\n",
    "    #        u'rt_units', u'detected_polarity', u'mz', u'mz_tolerance',u'mz_tolerance_units',\n",
    "    #         u'inchi', u'inchi_key', u'neutralized_2d_inchi', u'neutralized_2d_inchi_key', u'neutralized_inchi',\n",
    "    #        u'neutralized_inchi_key',u'chebi_id', u'hmdb_id', u'img_abc_id', u'kegg_id',u'lipidmaps_id', u'metacyc_id',\n",
    "    #        u'mono_isotopic_molecular_weight', u'pubchem_compound_id', u'kegg_url', u'chebi_url', u'hmdb_url', u'lipidmaps_url', u'pubchem_url',u'wikipedia_url',  u'source']\n",
    "\n",
    "    atlas_df = atlas_df[atlas_keys]\n",
    "    return atlas_df\n",
    "\n",
    "def get_data_for_mzrt(row,data_df_pos,data_df_neg,extra_time = 0.3,use_mz = 'mz',extra_mz = 0.0):\n",
    "    min_mz = '(%s >= %5.4f & '%(use_mz,row.mz - row.mz*row.mz_tolerance / 1e6 - extra_mz)\n",
    "    rt_min = 'rt >= %5.4f & '%(row.rt_min - extra_time)\n",
    "    rt_max = 'rt <= %5.4f & '%(row.rt_max + extra_time)\n",
    "    max_mz = '%s <= %5.4f)'%(use_mz,row.mz + row.mz*row.mz_tolerance / 1e6 + extra_mz)\n",
    "    ms1_query_str = '%s%s%s%s'%(min_mz,rt_min,rt_max,max_mz)\n",
    "    if row.detected_polarity == 'positive':\n",
    "        all_df = data_df_pos.query(ms1_query_str)\n",
    "    else:\n",
    "        all_df = data_df_neg.query(ms1_query_str)\n",
    "    return_df = pd.Series({'padded_feature_data':all_df.T.to_dense(),'in_feature':(all_df.rt >= row.rt_min) & (all_df.rt <= row.rt_max)})\n",
    "    return return_df\n",
    "\n",
    "def get_ms1_summary(row):\n",
    "    #A DataFrame of all points typically padded by \"extra time\"\n",
    "    all_df = row.padded_feature_data.T\n",
    "    \n",
    "    #slice out ms1 data that is NOT padded by extra_time\n",
    "    ms1_df = all_df[(row.in_feature == True)]#[['i','mz','polarity','rt']]\n",
    "\n",
    "    num_ms1_datapoints = ms1_df.shape[0]\n",
    "    if num_ms1_datapoints > 0:\n",
    "        idx = ms1_df.i.idxmax()\n",
    "        ms1_peak_df = ms1_df.ix[ms1_df['i'].idxmax()]\n",
    "        mz_peak = ms1_peak_df.mz\n",
    "        rt_peak = ms1_peak_df.rt\n",
    "        mz_centroid = sum(ms1_df.mz * ms1_df.i) / sum(ms1_df.i)\n",
    "        rt_centroid = sum(ms1_df.rt * ms1_df.i) / sum(ms1_df.i)\n",
    "        peak_height = ms1_peak_df.i\n",
    "        peak_area = sum(ms1_df.i)\n",
    "    else:\n",
    "        mz_peak = np.nan\n",
    "        rt_peak = np.nan\n",
    "        mz_centroid = np.nan\n",
    "        rt_centroid = np.nan\n",
    "        peak_height = np.nan\n",
    "        peak_area = np.nan\n",
    "        \n",
    "    return_df = pd.Series({ 'num_ms1_datapoints':num_ms1_datapoints,\n",
    "                            'mz_peak':mz_peak,\n",
    "                            'rt_peak':rt_peak,\n",
    "                            'mz_centroid':mz_centroid,\n",
    "                            'rt_centroid':rt_centroid,\n",
    "                            'peak_height':peak_height,\n",
    "                            'peak_area':peak_area})\n",
    "    \n",
    "    return return_df\n",
    "\n",
    "def get_ms2_data(row):\n",
    "    #A DataFrame of all points typically padded by \"extra time\"\n",
    "    all_df = row.padded_feature_data.T\n",
    "    \n",
    "    #slice out ms2 data that is NOT padded by extra_time\n",
    "    ms2_df = all_df[(row.in_feature == True)]#[['collision_energy','i','mz','polarity','precursor_MZ','precursor_intensity','rt']]\n",
    "\n",
    "    num_ms2_datapoints = ms2_df.shape[0]\n",
    "        \n",
    "    return_df = pd.Series({'ms2_datapoints':ms2_df.T.to_dense(),\n",
    "                            'num_ms2_datapoints':num_ms2_datapoints})\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def prefilter_ms1_dataframe_with_boundaries(data_df,rt_max,rt_min,mz_min,mz_max,extra_time = 1, extra_mz = 1):\n",
    "    if data_df.shape[0]==0:\n",
    "        return []\n",
    "    prefilter_query_str = 'rt < %5.4f & rt > %5.4f & mz > %5.4f & mz < %5.4f'%(rt_max+extra_time,rt_min-extra_time,mz_min - extra_mz, mz_max+extra_mz)\n",
    "    new_df = data_df.query(prefilter_query_str)\n",
    "    return new_df\n",
    "\n",
    "def get_ms1_eic(row):\n",
    "    #A DataFrame of all points typically padded by \"extra time\"\n",
    "    all_df = row.padded_feature_data.T\n",
    "    ms1_df = all_df[['i','mz','rt']]\n",
    "    ms1_df = ms1_df.sort_values('rt',ascending=True)\n",
    "    return pd.Series({'eic':ms1_df.T.to_dense()})\n",
    "\n",
    "\n",
    "def retrieve_most_intense_msms_scan(data):\n",
    "    import numpy as np\n",
    "    urt,idx = np.unique(data['rt'],return_index=True)\n",
    "    sx = np.argsort(data['precursor_intensity'][idx])[::-1]\n",
    "    prt = data['rt'][idx[sx]]\n",
    "    pmz = data['precursor_MZ'][idx[sx]]\n",
    "    pintensity = data['precursor_intensity'][idx[sx]]\n",
    "    #setup data format for searching\n",
    "    msms_data = {}\n",
    "    msms_data['spectra'] = []\n",
    "    msms_data['precursor_mz'] = []\n",
    "    msms_data['precursor_intensity'] = []\n",
    "    idx = np.argwhere((data['precursor_MZ'] == pmz[0]) & (data['rt'] == prt[0] )).flatten()\n",
    "    arr = np.array([data['mz'][idx], data['i'][idx]]).T\n",
    "    msms_data['spectra'] = arr\n",
    "    msms_data['precursor_mz'] = pmz\n",
    "    msms_data['precursor_intensity'] = pintensity\n",
    "    return msms_data\n",
    "\n",
    "def get_data_for_atlas_and_lcmsrun(atlas,df_container):\n",
    "    filtered_ms1_pos = prefilter_ms1_dataframe_with_boundaries(df_container['ms1_pos'],\n",
    "                                                               atlas_df[atlas_df.detected_polarity == 'positive'].rt_max.max(),\n",
    "                                                               atlas_df[atlas_df.detected_polarity == 'positive'].rt_min.min(),\n",
    "                                                               atlas_df[atlas_df.detected_polarity == 'positive'].mz.min(),\n",
    "                                                               atlas_df[atlas_df.detected_polarity == 'positive'].mz.max())\n",
    "    filtered_ms1_neg = prefilter_ms1_dataframe_with_boundaries(df_container['ms1_neg'],\n",
    "                                                           atlas_df[atlas_df.detected_polarity == 'negative'].rt_max.max(),\n",
    "                                                           atlas_df[atlas_df.detected_polarity == 'negative'].rt_min.min(),\n",
    "                                                           atlas_df[atlas_df.detected_polarity == 'negative'].mz.min(),\n",
    "                                                           atlas_df[atlas_df.detected_polarity == 'negative'].mz.max())\n",
    "\n",
    "    #here it needs to use the atlas.polarity to know which df to look in\n",
    "    ms1_feature_data = atlas_df.apply(lambda x: get_data_for_mzrt(x,filtered_ms1_pos,filtered_ms1_neg),axis=1)\n",
    "    ms1_summary = ms1_feature_data.apply(get_ms1_summary,axis=1)\n",
    "    ms1_eic = ms1_feature_data.apply(get_ms1_eic,axis=1)\n",
    "\n",
    "    #here it needs to use the atlas.polarity to know which df to look in\n",
    "    \n",
    "    filtered_ms2_pos = prefilter_ms1_dataframe_with_boundaries(df_container['ms2_pos'],\n",
    "                                                           atlas_df[atlas_df.detected_polarity == 'positive'].rt_max.max(),\n",
    "                                                           atlas_df[atlas_df.detected_polarity == 'positive'].rt_min.min(),\n",
    "                                                           0,\n",
    "                                                           atlas_df[atlas_df.detected_polarity == 'positive'].mz.max())\n",
    "    filtered_ms2_neg = prefilter_ms1_dataframe_with_boundaries(df_container['ms2_neg'],\n",
    "                                                           atlas_df[atlas_df.detected_polarity == 'negative'].rt_max.max(),\n",
    "                                                           atlas_df[atlas_df.detected_polarity == 'negative'].rt_min.min(),\n",
    "                                                           0,\n",
    "                                                           atlas_df[atlas_df.detected_polarity == 'negative'].mz.max())\n",
    "    ms2_feature_data = atlas_df.apply(lambda x: get_data_for_mzrt(x,filtered_ms2_pos,filtered_ms2_neg,use_mz = 'precursor_MZ',extra_mz = 0.01),axis=1)\n",
    "    ms2_data = ms2_feature_data.apply(get_ms2_data,axis=1)\n",
    "    dict_ms1_summary = [dict(row) for i,row in ms1_summary.iterrows()]\n",
    "    dict_eic = [row.eic.T.to_dict(orient='list') for i,row in ms1_eic.iterrows()]\n",
    "    for i,d in enumerate(dict_eic):\n",
    "        dict_eic[i]['intensity'] = dict_eic[i].pop('i')\n",
    "    dict_ms2 = [row.ms2_datapoints.T.to_dict(orient='list') for i,row in ms2_data.iterrows()]\n",
    "    return dict_ms1_summary,dict_eic,dict_ms2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160715_SK_SystemEval_QE119-1290_C18-LS-B15180-1to2_PosMS1_InjSK-MeOH_r01_s01rerun.mzML\n",
      "2016719_JGIQE_QCs_C18_pos\n"
     ]
    }
   ],
   "source": [
    "#These atlas/file throw an error because they don't have MSMS data\n",
    "# 20160715_SK_SystemEval_QE119-1290_C18-LS-B15180-1to2_PosMS1_InjSK-MeOH_r01_s01rerun.mzML\n",
    "# 2016719_JGIQE_QCs_C18_pos\n",
    "\n",
    "#get_data_for_atlas_and_lcmsrun(atlas,df_container)\n",
    "#has many steps that shouldn't be done if there is no data.\n",
    "\n",
    "# dict_ms1_summary,dict_eic,dict_ms2 = get_data_for_atlas_and_lcmsrun(atlas_df,df_container)\n",
    "print my_file.name\n",
    "print myAtlas.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20151016_pHILIC___NEG_MSMS_KZ_RootExuControl__1_of_2_MeOHExt__Run9.mzML 3.60179805756\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExuControl__2_of_2_MeOHExt__Run43.mzML 3.12154388428\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_3wk_1_of_4_MeOHExt__Run11.mzML 3.34261989594\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_3wk_2_of_4_MeOHExt__Run54.mzML 3.25781297684\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_3wk_3_of_4_MeOHExt__Run36.mzML 3.52949810028\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_3wk_4_of_4_MeOHExt__Run27.mzML 3.0998711586\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_6wk_1_of_4_MeOHExt__Run14.mzML 3.37020993233\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_6wk_2_of_4_MeOHExt__Run48.mzML 3.55708003044\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_6wk_3_of_4_MeOHExt__Run33.mzML 3.4470539093\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_6wk_4_of_4_MeOHExt__Run57.mzML 3.04358410835\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_9wk_1_of_4_MeOHExt__Run17.mzML 3.21957302094\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_9wk_2_of_4_MeOHExt__Run51.mzML 3.59547400475\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_9wk_3_of_4_MeOHExt__Run39.mzML 3.62876200676\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_9wk_4_of_4_MeOHExt__Run23.mzML 3.23821592331\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_12wk_1_of_4_MeOHExt__Run20.mzML 3.83145785332\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_12wk_2_of_4_MeOHExt__Run60.mzML 3.18092298508\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_12wk_3_of_4_MeOHExt__Run30.mzML 3.73835492134\n",
      "20151016_pHILIC___NEG_MSMS_KZ_RootExu_12wk_4_of_4_MeOHExt__Run45.mzML 3.40566897392\n"
     ]
    }
   ],
   "source": [
    "output_filename = '/global/homes/b/bpb/Downloads/KZ_Avena_Exudate_atlases_and groups_1/neg_data.pkl'\n",
    "atlas_df = make_atlas_df(myAtlas)\n",
    "data = []\n",
    "for my_group in groups:\n",
    "    for my_file in my_group.items:\n",
    "        t0 = time.time()\n",
    "        df_container = df_container_from_metatlas_file(my_file)\n",
    "        df_container = remove_ms1_data_not_in_atlas(atlas_df,df_container)\n",
    "\n",
    "        dict_ms1_summary,dict_eic,dict_ms2 = get_data_for_atlas_and_lcmsrun(atlas_df,df_container)\n",
    "        row = []\n",
    "        for i in range(len(dict_eic)):\n",
    "            result = {}\n",
    "            result['atlas_name'] = myAtlas.name\n",
    "            result['atlas_unique_id'] = myAtlas.unique_id\n",
    "            result['lcmsrun'] = my_file\n",
    "            result['group'] = my_group\n",
    "            temp_compound = copy.deepcopy(myAtlas.compound_identifications[i])\n",
    "            result['identification'] = temp_compound\n",
    "            result['data'] = {}\n",
    "            result['data']['eic'] = dict_eic[i]\n",
    "            result['data']['ms1_summary'] = dict_ms1_summary[i]\n",
    "            result['data']['msms'] = {}\n",
    "            if dict_ms2[i]['mz']:\n",
    "                for k in dict_ms2[0].keys():\n",
    "                    dict_ms2[i][k] = np.asarray(dict_ms2[i][k])\n",
    "#                 if temp_compound.mz_references[0].observed_polarity == 'positive':\n",
    "#                     dict_ms2[i]['polarity'] = dict_ms2[i]['mz'] * 0.0 + 1.0\n",
    "#                 else:\n",
    "#                     dict_ms2[i]['polarity'] = dict_ms2[i]['mz'] * 0.0\n",
    "                result['data']['msms']['data'] = dict_ms2[i]\n",
    "            else:\n",
    "                result['data']['msms']['data'] = []\n",
    "            row.append(result)\n",
    "        data.append(row)\n",
    "        print my_file.name,time.time() - t0\n",
    "with open(output_filename,'w') as f:\n",
    "    dill.dump(data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# result = {}\n",
    "# result['atlas_name'] = myAtlas.name\n",
    "# result['atlas_unique_id'] = myAtlas.unique_id\n",
    "# result['lcmsrun'] = treatment_groups.items[j]\n",
    "# result['group'] = treatment_groups\n",
    "# temp_compound = copy.deepcopy(compound)\n",
    "# result['identification'] = temp_compound\n",
    "\n",
    "# result['data'] = return_data...\n",
    "# return_data['ms1_summary']['polarity'] = polarity\n",
    "# return_data['ms1_summary']['mz_centroid'] = np.sum(np.multiply(ms1_data['i'],ms1_data['mz'])) / np.sum(ms1_data['i'])\n",
    "# return_data['ms1_summary']['rt_centroid'] = np.sum(np.multiply(ms1_data['i'],ms1_data['rt'])) / np.sum(ms1_data['i'])\n",
    "# idx = np.argmax(ms1_data['i'])\n",
    "# return_data['ms1_summary']['mz_peak'] = ms1_data['mz'][idx]\n",
    "# return_data['ms1_summary']['rt_peak'] = ms1_data['rt'][idx]        \n",
    "# return_data['ms1_summary']['peak_height'] = ms1_data['i'][idx]\n",
    "# return_data['ms1_summary']['peak_area'] = np.sum(ms1_data['i'])\n",
    "\n",
    "# return_data['msms']['most_intense_precursor'] = retrieve_most_intense_msms_scan(fragmentation_data)\n",
    "# return_data['msms']['data'] = fragmentation_data\n",
    "# return_data['msms']['polarity'] = polarity\n",
    "\n",
    "# return_data['eic']['rt'] = rt\n",
    "# return_data['eic']['intensity'] = intensity\n",
    "# return_data['eic']['polarity'] = polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.concat([atlas_df,temp,temp2],axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_for_groups_and_atlas(group,myAtlas,output_filename,use_set1 = False):\n",
    "    \"\"\"\n",
    "    get and pickle everything This is MSMS, raw MS1 datapoints, compound, group info, and file info\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    import copy as copy\n",
    "    for i,treatment_groups in enumerate(group):\n",
    "        for j in range(len(treatment_groups.items)):\n",
    "            myFile = treatment_groups.items[j].hdf5_file\n",
    "    #         try:\n",
    "    #             rt_reference_index = int(treatment_groups.name[-1]) - 1\n",
    "    #         except:\n",
    "    #             rt_reference_index = 3\n",
    "            print i,len(group),myFile\n",
    "            row = []\n",
    "            for compound in myAtlas.compound_identifications:\n",
    "                result = {}\n",
    "                result['atlas_name'] = myAtlas.name\n",
    "                result['atlas_unique_id'] = myAtlas.unique_id\n",
    "                result['lcmsrun'] = treatment_groups.items[j]\n",
    "                result['group'] = treatment_groups\n",
    "                temp_compound = copy.deepcopy(compound)\n",
    "                if use_set1:\n",
    "                    if '_Set1' in treatment_groups.name:\n",
    "                        temp_compound.rt_references[0].rt_min -= 0.2\n",
    "                        temp_compound.rt_references[0].rt_max -= 0.2\n",
    "                        temp_compound.rt_references[0].rt_peak -= 0.2\n",
    "                    temp_compound.mz_references[0].mz_tolerance = 20\n",
    "                result['identification'] = temp_compound\n",
    "                result['data'] = ma_data.get_data_for_a_compound(temp_compound.mz_references[0],\n",
    "                                        temp_compound.rt_references[0],\n",
    "                                        [ 'ms1_summary', 'eic', 'msms' ],\n",
    "                                        myFile,0.2)\n",
    "    #                 print result['data']['ms1_summary']\n",
    "                row.append(result)\n",
    "            data.append(row)\n",
    "        with open(output_filename,'w') as f:\n",
    "            dill.dump(data,f)\n",
    "import numpy as np\n",
    "import os.path\n",
    "import sys\n",
    "import tables\n",
    "def get_unique_scan_data(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    data - numpy nd array containing MSMS data\n",
    "    \n",
    "    Output:\n",
    "    rt - retention time of scan\n",
    "    pmz - precursor m/z of scan\n",
    "    Both are sorted by descending precursor ion intensity\n",
    "    \n",
    "    for data returned from h5query.get_data(),\n",
    "    return the retention time and precursor m/z\n",
    "    sorted by descending precursor ion intensity\n",
    "    \"\"\"\n",
    "    urt,idx = np.unique(data['rt'],return_index=True)\n",
    "    sx = np.argsort(data['precursor_intensity'][idx])[::-1]\n",
    "    prt = data['rt'][idx[sx]]\n",
    "    pmz = data['precursor_MZ'][idx[sx]]\n",
    "    pintensity = data['precursor_intensity'][idx[sx]]\n",
    "    return prt,pmz,pintensity\n",
    "\n",
    "\n",
    "def get_non_redundant_precursor_list(prt,pmz,rt_cutoff,mz_cutoff):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    rt - retention time of scan\n",
    "    pmz - precursor m/z of scan\n",
    "    Both are sorted by descending precursor ion intensity\n",
    "    rt_cutoff - \n",
    "    mz_cutoff - \n",
    "    \n",
    "    Output:\n",
    "    list_of_prt - list of \n",
    "    list_of_pmz - list of \n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_pmz = [] #contains list of precursor m/z [pmz1,pmz2,...,pmz_n]\n",
    "    list_of_prt = [] #contains list of precursor rt [prt1,prt2,...,prt_n]\n",
    "    \n",
    "    for i in range(len(prt)):\n",
    "        if len(list_of_pmz) == 0:\n",
    "            # none are in the list yet; so there is nothing to check\n",
    "            list_of_pmz.append(pmz[i])\n",
    "            list_of_prt.append(prt[i])\n",
    "        else:\n",
    "            # check if new rt qualifies for inclusion\n",
    "            if min(abs(list_of_prt - prt[i])) > rt_cutoff or min(abs(list_of_pmz - pmz[i])) > mz_cutoff:\n",
    "                list_of_pmz.append(pmz[i])\n",
    "                list_of_prt.append(prt[i])\n",
    "    return list_of_prt,list_of_pmz\n",
    "\n",
    "\n",
    "def organize_msms_scan_data(data,list_of_prt,list_of_pmz,list_of_pintensity):\n",
    "    #setup data format for searching\n",
    "    msms_data = {}\n",
    "    msms_data['spectra'] = []\n",
    "    msms_data['precursor_mz'] = []\n",
    "    msms_data['precursor_rt'] = []\n",
    "    msms_data['precursor_intensity'] = []\n",
    "    for i,(prt,pmz,pintensity) in enumerate(zip(list_of_prt,list_of_pmz,list_of_pintensity)):\n",
    "        idx = np.argwhere((data['precursor_MZ'] == pmz) & (data['rt'] == prt )).flatten()\n",
    "        arr = np.array([data['mz'][idx], data['i'][idx]]).T\n",
    "        msms_data['spectra'].append(arr)\n",
    "        msms_data['precursor_mz'].append(pmz)\n",
    "        msms_data['precursor_rt'].append(prt)\n",
    "        msms_data['precursor_intensity'].append(pintensity)\n",
    "    return msms_data\n",
    "\n",
    "def retrieve_most_intense_msms_scan(data):\n",
    "    import numpy as np\n",
    "    urt,idx = np.unique(data['rt'],return_index=True)\n",
    "    sx = np.argsort(data['precursor_intensity'][idx])[::-1]\n",
    "    prt = data['rt'][idx[sx]]\n",
    "    pmz = data['precursor_MZ'][idx[sx]]\n",
    "    pintensity = data['precursor_intensity'][idx[sx]]\n",
    "    #setup data format for searching\n",
    "    msms_data = {}\n",
    "    msms_data['spectra'] = []\n",
    "    msms_data['precursor_mz'] = []\n",
    "    msms_data['precursor_intensity'] = []\n",
    "    idx = np.argwhere((data['precursor_MZ'] == pmz[0]) & (data['rt'] == prt[0] )).flatten()\n",
    "    arr = np.array([data['mz'][idx], data['i'][idx]]).T\n",
    "    msms_data['spectra'] = arr\n",
    "    msms_data['precursor_mz'] = pmz\n",
    "    msms_data['precursor_intensity'] = pintensity\n",
    "    return msms_data\n",
    "\n",
    "\n",
    "def get_data_for_a_compound(mz_ref,rt_ref,what_to_get,h5file,extra_time):\n",
    "    \"\"\"\n",
    "    A helper function to query the various metatlas data selection \n",
    "    commands for a compound defined in an experimental atlas.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    MZref : a MetAtlas Object for a m/z reference Class\n",
    "        this contains the m/z, m/z tolerance, and tolerance units to slice the m/z dimension\n",
    "    RTref : a MetAtlas Object for a retention time reference Class\n",
    "        this contains the rt min, max, peak, and units to slice the retention time dimension\n",
    "    what_to_get : a list of strings\n",
    "        this contains one or more of [ 'ms1_summary', 'eic', '2dhist', 'msms' ]\n",
    "    h5_file : str\n",
    "        Path to input_file\n",
    "    polarity : int\n",
    "        [0 or 1] for negative or positive ionzation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    #TODO : polarity should be handled in the experiment and not a loose parameter\n",
    "    import numpy as np\n",
    "    from metatlas import h5_query as h5q\n",
    "    import tables\n",
    "    \n",
    "    #get a pointer to the hdf5 file\n",
    "    fid = tables.open_file(h5file) #TODO: should be a \"with open:\"\n",
    "\n",
    "    if mz_ref.detected_polarity  == 'positive':\n",
    "        polarity = 1\n",
    "    else:\n",
    "        polarity = 0\n",
    "        \n",
    "    \n",
    "    mz_theor = mz_ref.mz\n",
    "    if mz_ref.mz_tolerance_units  == 'ppm': #convert to ppm\n",
    "        ppm_uncertainty = mz_ref.mz_tolerance\n",
    "    else:\n",
    "        ppm_uncertainty = mz_ref.mz_tolerance / mz_ref.mz * 1e6\n",
    "        \n",
    "#     if 'min' in rt_ref.rt_units: #convert to seconds\n",
    "#     rt_min = rt_ref.rt_min / 60\n",
    "#     rt_max = rt_ref.rt_max / 60\n",
    "#     else:\n",
    "    rt_min = rt_ref.rt_min\n",
    "    rt_max = rt_ref.rt_max\n",
    "        \n",
    "    mz_min = mz_theor - mz_theor * ppm_uncertainty / 1e6\n",
    "    mz_max = mz_theor + mz_theor * ppm_uncertainty / 1e6\n",
    "    \n",
    "    return_data = {}\n",
    "    \n",
    "    if 'ms1_summary' in what_to_get:\n",
    "        #Get Summary Data\n",
    "        \n",
    "        #First get MS1 Raw Data\n",
    "        ms_level=1\n",
    "        return_data['ms1_summary'] = {}\n",
    "        try:\n",
    "            ms1_data = h5q.get_data(fid, \n",
    "                                     ms_level=1,\n",
    "                                     polarity=polarity,\n",
    "                                     min_mz=mz_min,\n",
    "                                     max_mz=mz_max,\n",
    "                                     min_rt=rt_min,\n",
    "                                     max_rt=rt_max)\n",
    "\n",
    "            return_data['ms1_summary']['polarity'] = polarity\n",
    "            return_data['ms1_summary']['mz_centroid'] = np.sum(np.multiply(ms1_data['i'],ms1_data['mz'])) / np.sum(ms1_data['i'])\n",
    "            return_data['ms1_summary']['rt_centroid'] = np.sum(np.multiply(ms1_data['i'],ms1_data['rt'])) / np.sum(ms1_data['i'])\n",
    "            idx = np.argmax(ms1_data['i'])\n",
    "            return_data['ms1_summary']['mz_peak'] = ms1_data['mz'][idx]\n",
    "            return_data['ms1_summary']['rt_peak'] = ms1_data['rt'][idx]        \n",
    "            return_data['ms1_summary']['peak_height'] = ms1_data['i'][idx]\n",
    "            return_data['ms1_summary']['peak_area'] = np.sum(ms1_data['i'])\n",
    "\n",
    "        except:\n",
    "            return_data['ms1_summary']['polarity'] = []\n",
    "            return_data['ms1_summary']['mz_centroid'] = []\n",
    "            return_data['ms1_summary']['rt_centroid'] = []\n",
    "            return_data['ms1_summary']['mz_peak'] = []\n",
    "            return_data['ms1_summary']['rt_peak'] = []\n",
    "            return_data['ms1_summary']['peak_height'] = []\n",
    "            return_data['ms1_summary']['peak_area'] = []\n",
    "\n",
    "    \n",
    "    if 'eic' in what_to_get:\n",
    "        #Get Extracted Ion Chromatogram\n",
    "        # TODO : If a person calls for summary, then they will already have the MS1 raw data\n",
    "        return_data['eic'] = {}\n",
    "        try:\n",
    "            rt,intensity = h5q.get_chromatogram(fid, mz_min, mz_max, ms_level=ms_level, polarity=polarity, min_rt = rt_min - extra_time, max_rt = rt_max + extra_time)\n",
    "            return_data['eic']['rt'] = rt\n",
    "            return_data['eic']['intensity'] = intensity\n",
    "            return_data['eic']['polarity'] = polarity\n",
    "\n",
    "        except:    \n",
    "            return_data['eic']['rt'] = []\n",
    "            return_data['eic']['intensity'] = []\n",
    "            return_data['eic']['polarity'] = []\n",
    "    \n",
    "    if '2dhist' in what_to_get:\n",
    "        #Get 2D histogram of intensity values in m/z and retention time\n",
    "        mzEdges = np.logspace(np.log10(100),np.log10(1000),10000)\n",
    "#         mzEdges = np.linspace(mz_theor - 3, mz_theor + 30,100) #TODO : number of mz bins should be an optional parameter\n",
    "        rtEdges = np.linspace(rt_min,rt_max,100) #TODO : number of rt bins should be an optional parameter. When not provided, it shoulddefauly to unique bins\n",
    "        ms_level = 1 #TODO : ms_level should be a parameter\n",
    "        return_data['2dhist'] = {}\n",
    "        return_data['2dhist'] = h5q.get_heatmap(fid,mzEdges,rtEdges,ms_level,polarity)\n",
    "        return_data['2dhist']['polarity'] = polarity\n",
    "    \n",
    "    if 'msms' in what_to_get:\n",
    "        #Get Fragmentation Data\n",
    "        ms_level=2\n",
    "        return_data['msms'] = {}\n",
    "        try:\n",
    "            fragmentation_data = h5q.get_data(fid, \n",
    "                                     ms_level=ms_level,\n",
    "                                     polarity=polarity,\n",
    "                                     min_mz=0,\n",
    "                                     max_mz=mz_theor+2,#TODO : this needs to be a parameter\n",
    "                                     min_rt=rt_min,\n",
    "                                     max_rt=rt_max,\n",
    "                                     min_precursor_MZ=mz_min - 0.015,\n",
    "                                     max_precursor_MZ=mz_max + 0.015) #Add the 0.01 because Thermo doesn't store accurate precursor m/z\n",
    "        #                     min_precursor_intensity=0, #TODO : this needs to be a parameter\n",
    "        #                     max_precursor_intensity=0,#TODO : this needs to be a parameter\n",
    "        #                     min_collision_energy=0,#TODO : this needs to be a parameter\n",
    "        #                     max_collision_energy=0)#TODO : this needs to be a parameter\n",
    "    #         prt,pmz = get_unique_scan_data(fragmentation_data)\n",
    "    #         rt_cutoff = 0.23\n",
    "    #         mz_cutoff = 0.05\n",
    "    #         list_of_prt,list_of_pmz = get_non_redundant_precursor_list(prt,pmz,rt_cutoff,mz_cutoff)\n",
    "    #         return_data['msms']['data'] = organize_msms_scan_data(fragmentation_data,list_of_prt,list_or_pmz)\n",
    "            return_data['msms']['most_intense_precursor'] = retrieve_most_intense_msms_scan(fragmentation_data)\n",
    "            return_data['msms']['data'] = fragmentation_data\n",
    "            return_data['msms']['polarity'] = polarity\n",
    "        except:\n",
    "            return_data['msms']['most_intense_precursor'] = []\n",
    "            return_data['msms']['data'] = []\n",
    "            return_data['msms']['polarity'] = []\n",
    "            \n",
    "    fid.close() #close the file\n",
    "    return return_data\n",
    "\n",
    "\n",
    "\n",
    "def get_dill_data(fname):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: dill file name\n",
    "\n",
    "    Returns a list containing the data present in the dill file\n",
    "    -------\n",
    "    \"\"\"\n",
    "    import dill\n",
    "\n",
    "    data = list()\n",
    "    \n",
    "    if os.path.exists(fname):\n",
    "        with open(fname,'r') as f:\n",
    "            try:\n",
    "                data = dill.load(f)\n",
    "            except IOError as e:\n",
    "                print \"I/O error({0}): {1}\".format(e.errno, e.strerror)\n",
    "            except:  # handle other exceptions such as attribute errors\n",
    "                print \"Unexpected error:\", sys.exc_info()[0]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_group_names(data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: either a file name (str) or a list generated from loading the dill file\n",
    "\n",
    "    Returns list containing the group names present in the dill file\n",
    "    -------\n",
    "    \"\"\"\n",
    "\n",
    "    # if data is a string then it's a file name - get its data\n",
    "    if isinstance(data, basestring):\n",
    "        data = get_dill_data(data)\n",
    "\n",
    "    group_names = list()\n",
    "    for i,d in enumerate(data):\n",
    "        group_names.append(d[0]['group'].name)\n",
    "\n",
    "    return group_names\n",
    "\n",
    "\n",
    "def get_file_names(data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: either a file name (str) or a list generated from loading the dill file\n",
    "\n",
    "    Returns list containing the hdf file names present in the dill file\n",
    "    -------\n",
    "    \"\"\"\n",
    "    import os.path\n",
    "\n",
    "    # if data is a string then it's a file name - get its data\n",
    "    if isinstance(data, basestring):\n",
    "        data = get_dill_data(data)\n",
    "\n",
    "    file_names = list()\n",
    "    for i,d in enumerate(data):\n",
    "        file_names.append(os.path.basename(d[0]['lcmsrun'].hdf5_file))\n",
    "\n",
    "    return file_names\n",
    "\n",
    "\n",
    "def get_compound_names(data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: either a file name (str) or a list generated from loading the dill file\n",
    "\n",
    "    Returns a tuple of lists containing the compound names and compound objects present in the dill file\n",
    "    -------\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    import re\n",
    "\n",
    "    # if data is a string then it's a file name - get its data\n",
    "    if isinstance(data, basestring):\n",
    "        data = get_dill_data(data)\n",
    "\n",
    "    compound_names = list()\n",
    "    compound_objects = list()\n",
    "\n",
    "    for i,d in enumerate(data[0]):\n",
    "        compound_objects.append(d['identification'])\n",
    "        if len(d['identification'].compound) > 0:\n",
    "            _str = d['identification'].compound[0].name\n",
    "        else:\n",
    "            _str = d['identification'].name\n",
    "        _str = _str.split('///')[0]\n",
    "        newstr = '%s_%s_%s_%5.2f'%(_str,d['identification'].mz_references[0].detected_polarity,\n",
    "                d['identification'].mz_references[0].adduct,d['identification'].rt_references[0].rt_peak)\n",
    "        newstr = re.sub('\\.', 'p', newstr) #2 or more in regexp\n",
    "\n",
    "        newstr = re.sub('[\\[\\]]','',newstr)\n",
    "        newstr = re.sub('[^A-Za-z0-9+-]+', '_', newstr)\n",
    "        newstr = re.sub('i_[A-Za-z]+_i_', '', newstr)\n",
    "        if newstr[0] == '_':\n",
    "            newstr = newstr[1:]\n",
    "        if newstr[0] == '-':\n",
    "            newstr = newstr[1:]\n",
    "        if newstr[-1] == '_':\n",
    "            newstr = newstr[:-1]\n",
    "\n",
    "        newstr = re.sub('[^A-Za-z0-9]{2,}', '', newstr) #2 or more in regexp\n",
    "        compound_names.append(newstr)\n",
    "\n",
    "    # If duplicate compound names exist, then append them with a number\n",
    "    D = defaultdict(list)\n",
    "    for i,item in enumerate(compound_names):\n",
    "        D[item].append(i)\n",
    "    D = {k:v for k,v in D.items() if len(v)>1}\n",
    "    for k in D.keys():\n",
    "        for i,f in enumerate(D[k]):\n",
    "            compound_names[f] = '%s%d'%(compound_names[f],i)\n",
    "\n",
    "    return (compound_names, compound_objects)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
